{"metadata":{"colab":{"authorship_tag":"ABX9TyO1TQgyoY95QEMQO8RWwCM5","collapsed_sections":["cZoexHVhCLgd","SM__P9GNCFzY","dJbB_hU0CJH8"],"name":"","version":""},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9307769,"sourceType":"datasetVersion","datasetId":5636633}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Dizionari","metadata":{"id":"cZoexHVhCLgd"}},{"cell_type":"code","source":"!pip install sentence_transformers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8296,"status":"ok","timestamp":1725355969031,"user":{"displayName":"Massimo Trippetta","userId":"11659441013912278867"},"user_tz":-120},"id":"WT_NAuUdB0n8","outputId":"9b77ecf2-35fa-491c-fca2-9405af070f6c","execution":{"iopub.status.busy":"2024-09-09T14:26:50.488656Z","iopub.execute_input":"2024-09-09T14:26:50.489528Z","iopub.status.idle":"2024-09-09T14:27:04.571395Z","shell.execute_reply.started":"2024-09-09T14:26:50.489486Z","shell.execute_reply":"2024-09-09T14:27:04.570481Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting sentence_transformers\n  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.44.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.14.0)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.24.6)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence_transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-3.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install langchain","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"onicWW1oB5j3","outputId":"2d3aa254-111b-4162-efc2-55b11c2c911a","execution":{"iopub.status.busy":"2024-09-09T14:27:04.573318Z","iopub.execute_input":"2024-09-09T14:27:04.573675Z","iopub.status.idle":"2024-09-09T14:27:20.237568Z","shell.execute_reply.started":"2024-09-09T14:27:04.573620Z","shell.execute_reply":"2024-09-09T14:27:20.236399Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting langchain\n  Downloading langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.3.0,>=0.2.38 (from langchain)\n  Downloading langchain_core-0.2.38-py3-none-any.whl.metadata (6.2 kB)\nCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.116-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.8.2)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (1.33)\nCollecting packaging<25,>=23.2 (from langchain-core<0.3.0,>=0.2.38->langchain)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (4.12.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.20.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.7.4)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain) (2.4)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.0)\nDownloading langchain-0.2.16-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.2.38-py3-none-any.whl (396 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\nDownloading langsmith-0.1.116-py3-none-any.whl (290 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, langsmith, langchain-core, langchain-text-splitters, langchain\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.8.2 requires cubinlinker, which is not installed.\ncudf 24.8.2 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.2 requires ptxcompiler, which is not installed.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.8.2 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.2 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ndistributed 2024.7.1 requires dask==2024.7.1, but you have dask 2024.8.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.4 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.4 requires shapely<2.1,>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.8.1 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.9.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.2.16 langchain-core-0.2.38 langchain-text-splitters-0.2.4 langsmith-0.1.116 packaging-24.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install langchain_community","metadata":{"colab":{"background_save":true},"id":"YVBlxNn3Dhzb","execution":{"iopub.status.busy":"2024-09-09T14:29:32.269046Z","iopub.execute_input":"2024-09-09T14:29:32.269411Z","iopub.status.idle":"2024-09-09T14:29:47.332352Z","shell.execute_reply.started":"2024-09-09T14:29:32.269378Z","shell.execute_reply":"2024-09-09T14:29:47.331147Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting langchain_community\n  Using cached langchain_community-0.2.16-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (3.9.5)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.6.7)\nRequirement already satisfied: langchain<0.3.0,>=0.2.16 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.2.16)\nRequirement already satisfied: langchain-core<0.3.0,>=0.2.38 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.2.38)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.1.116)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (1.26.4)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (8.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\nRequirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.16->langchain_community) (0.2.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.16->langchain_community) (2.8.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain_community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain_community) (24.1)\nRequirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain_community) (4.12.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (0.27.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2024.7.4)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain_community) (2.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain_community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain_community) (2.20.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (1.2.0)\nUsing cached langchain_community-0.2.16-py3-none-any.whl (2.3 MB)\nInstalling collected packages: langchain_community\nSuccessfully installed langchain_community-0.2.16\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install langchain_openai","metadata":{"colab":{"background_save":true},"id":"mE7e_Yq1Dj2z","execution":{"iopub.status.busy":"2024-09-09T14:29:47.334359Z","iopub.execute_input":"2024-09-09T14:29:47.334738Z","iopub.status.idle":"2024-09-09T14:30:01.765285Z","shell.execute_reply.started":"2024-09-09T14:29:47.334698Z","shell.execute_reply":"2024-09-09T14:30:01.764170Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Collecting langchain_openai\n  Downloading langchain_openai-0.1.23-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: langchain-core<0.3.0,>=0.2.35 in /opt/conda/lib/python3.10/site-packages (from langchain_openai) (0.2.38)\nCollecting openai<2.0.0,>=1.40.0 (from langchain_openai)\n  Downloading openai-1.44.0-py3-none-any.whl.metadata (22 kB)\nCollecting tiktoken<1,>=0.7 (from langchain_openai)\n  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.35->langchain_openai) (6.0.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.35->langchain_openai) (1.33)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.75 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.35->langchain_openai) (0.1.116)\nRequirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.35->langchain_openai) (24.1)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.35->langchain_openai) (2.8.2)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.35->langchain_openai) (8.3.0)\nRequirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.35->langchain_openai) (4.12.2)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.4.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (0.27.0)\nCollecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain_openai)\n  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.66.4)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (3.7)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (2024.7.4)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.35->langchain_openai) (2.4)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.35->langchain_openai) (3.10.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.35->langchain_openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.35->langchain_openai) (2.20.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (1.26.18)\nDownloading langchain_openai-0.1.23-py3-none-any.whl (51 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading openai-1.44.0-py3-none-any.whl (367 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.8/367.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: jiter, tiktoken, openai, langchain_openai\nSuccessfully installed jiter-0.5.0 langchain_openai-0.1.23 openai-1.44.0 tiktoken-0.7.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install langchain_huggingface","metadata":{"colab":{"background_save":true},"id":"OU3KeBYNDlrV","execution":{"iopub.status.busy":"2024-09-09T14:30:01.766953Z","iopub.execute_input":"2024-09-09T14:30:01.767759Z","iopub.status.idle":"2024-09-09T14:30:14.949256Z","shell.execute_reply.started":"2024-09-09T14:30:01.767706Z","shell.execute_reply":"2024-09-09T14:30:14.948101Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Collecting langchain_huggingface\n  Using cached langchain_huggingface-0.0.3-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langchain_huggingface) (0.24.6)\nRequirement already satisfied: langchain-core<0.3,>=0.1.52 in /opt/conda/lib/python3.10/site-packages (from langchain_huggingface) (0.2.38)\nRequirement already satisfied: sentence-transformers>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from langchain_huggingface) (3.0.1)\nRequirement already satisfied: tokenizers>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from langchain_huggingface) (0.19.1)\nRequirement already satisfied: transformers>=4.39.0 in /opt/conda/lib/python3.10/site-packages (from langchain_huggingface) (4.44.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.33)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.75 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.1.116)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (2.8.2)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (8.3.0)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.14.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (9.5.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.0->langchain_huggingface) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.4)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain_huggingface) (2.4)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.27.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (3.10.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_huggingface) (2.20.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.7.4)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.14.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.2.0)\nUsing cached langchain_huggingface-0.0.3-py3-none-any.whl (17 kB)\nInstalling collected packages: langchain_huggingface\nSuccessfully installed langchain_huggingface-0.0.3\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install chromadb","metadata":{"colab":{"background_save":true},"id":"neWYZjqmFYgN","execution":{"iopub.status.busy":"2024-09-09T14:30:14.951664Z","iopub.execute_input":"2024-09-09T14:30:14.952016Z","iopub.status.idle":"2024-09-09T14:30:28.894497Z","shell.execute_reply.started":"2024-09-09T14:30:14.951977Z","shell.execute_reply":"2024-09-09T14:30:28.893280Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Requirement already satisfied: chromadb in /opt/conda/lib/python3.10/site-packages (0.5.5)\nRequirement already satisfied: build>=1.0.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.2.2)\nRequirement already satisfied: pydantic>=1.9 in /opt/conda/lib/python3.10/site-packages (from chromadb) (2.8.2)\nRequirement already satisfied: chroma-hnswlib==0.7.6 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.7.6)\nRequirement already satisfied: fastapi>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.111.0)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.1)\nRequirement already satisfied: numpy<2.0.0,>=1.22.5 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.26.4)\nRequirement already satisfied: posthog>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (3.6.4)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.12.2)\nRequirement already satisfied: onnxruntime>=1.14.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.19.2)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.25.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.25.0)\nRequirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.46b0)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.25.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.19.1)\nRequirement already satisfied: pypika>=0.48.9 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.48.9)\nRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.66.4)\nRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (7.7.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.4.0)\nRequirement already satisfied: grpcio>=1.58.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.62.2)\nRequirement already satisfied: bcrypt>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.2.0)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.12.3)\nRequirement already satisfied: kubernetes>=28.1.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (30.1.0)\nRequirement already satisfied: tenacity>=8.2.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (8.3.0)\nRequirement already satisfied: PyYAML>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.0.2)\nRequirement already satisfied: mmh3>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.1.0)\nRequirement already satisfied: orjson>=3.9.12 in /opt/conda/lib/python3.10/site-packages (from chromadb) (3.10.4)\nRequirement already satisfied: httpx>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.27.0)\nRequirement already satisfied: packaging>=19.1 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (24.1)\nRequirement already satisfied: pyproject_hooks in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (1.1.0)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\nRequirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\nRequirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.0.4)\nRequirement already satisfied: jinja2>=2.11.2 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (3.1.4)\nRequirement already satisfied: python-multipart>=0.0.7 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.0.9)\nRequirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (5.10.0)\nRequirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (2.1.1)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (4.4.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (2024.7.4)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (3.7)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.30.0)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\nRequirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\nRequirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nRequirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.18)\nRequirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.2)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\nRequirement already satisfied: importlib-metadata<=7.1,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.0)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.1)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.25.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\nRequirement already satisfied: opentelemetry-proto==1.25.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\nRequirement already satisfied: opentelemetry-instrumentation-asgi==0.46b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\nRequirement already satisfied: opentelemetry-instrumentation==0.46b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\nRequirement already satisfied: opentelemetry-util-http==0.46b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\nRequirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (70.0.0)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\nRequirement already satisfied: asgiref~=3.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\nRequirement already satisfied: monotonic>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\nRequirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (2.20.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.24.6)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (13.7.1)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\nRequirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb) (2.6.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.18.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.0)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport json\nimport re\nimport nltk\n#nltk.download('wordnet')\n#nltk.download('stopwords')\n#nltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom gensim.models.phrases import Phrases\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm\nimport numpy as np\nfrom transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.schema import Document\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\nimport os\nfrom langchain import hub\nfrom langchain_huggingface import HuggingFaceEmbeddings\nimport chromadb\nimport openai\nfrom langchain.schema import HumanMessage, SystemMessage\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration","metadata":{"id":"nZfSrDiEBWZA","execution":{"iopub.status.busy":"2024-09-09T14:30:28.895882Z","iopub.execute_input":"2024-09-09T14:30:28.896216Z","iopub.status.idle":"2024-09-09T14:30:30.914630Z","shell.execute_reply.started":"2024-09-09T14:30:28.896178Z","shell.execute_reply":"2024-09-09T14:30:30.913897Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Carico dati","metadata":{"id":"SM__P9GNCFzY"}},{"cell_type":"code","source":"# Carico il file json\nfile_path = '/kaggle/input/arxivdata-json/arxivData.json'\n\nwith open(file_path, 'r') as file:\n    data = json.load(file)\n\ndf = pd.DataFrame(data)\ndf = df[['author', 'summary', 'tag', 'title', 'year']]\ndf[\"summary\"] = df[\"summary\"].map(lambda x : x.replace(\"\\n\", \" \"))\ndf['year'] = df['year'].astype(str)\n#print(df.head())\n#print(df.columns)\n\n\n# ESTRAGGO LE TAG\ndef clean_tags(x):\n    x = re.sub(r'\\[', '', x)\n    x = re.sub(r'\\{', '', x)\n    x = re.sub(r\"'term': '\", '', x)\n    y = re.sub(r\"'\", '', x)\n    y = y.strip()\n    return y\n\n# Rinomino i tag\ntag_abbreviation = [\n    \"cs.CV\",\n    \"cs.LG\",\n    \"cs.AI\",\n    \"stat.ML\",\n    \"cs.CL\",\n    \"cs.NE\",\n    \"cs.IR\",\n    \"math.OC\",\n    \"cs.RO\",\n    \"ph\",\n    \"cs.LO\"\n]\n\ntag_description = [\n    \"Computer Vision and Pattern Recognition\",\n    \"Machine Learning\",\n    \"Artificial Intelligence\",\n    \"Statistical Learning\",\n    \"Computation and Language\",\n    \"Neural and Evolutionary Computing\",\n    \"Information Retrieval\",\n    \"Optimization and Control\",\n    \"Robotics\",\n    \"High Energy Physics\",\n    \"Logic in Computer Science\"\n]\n\ntag_translation_dict = dict(zip(tag_abbreviation, tag_description))\n\ndef get_tags(x):\n    a = re.split(r',', x)\n    b = [re.search(r'term', item) is not None for item in a]\n    c = [a[i] for i in range(len(a)) if b[i]]\n    d = [clean_tags(item) for item in c]\n    translated_tags = [tag_translation_dict.get(tag, tag) for tag in d]\n    return translated_tags\n\ndf['tag2'] = df['tag'].apply(get_tags)\n\nall_tags = [tag for sublist in df['tag2'] for tag in sublist]\n\n# ESTRAGGO GLI AUTORI\ndef clean_authors(x):\n    try:\n        authors_list = json.loads(x.replace(\"'\", '\"'))\n        names = [author['name'] for author in authors_list]\n        cleaned_authors = ', '.join(names)\n        return cleaned_authors\n    except json.JSONDecodeError:\n        return x\ndf[\"author\"] = df[\"author\"].apply(clean_authors)\n\n# passiamo alla creazione della RAG, possiamo concatenare il summmary con il titolo in modo da dare più informazioni al modello\ndf['tag2_str'] = df['tag2'].apply(lambda x: ', '.join(x) if isinstance(x, list) else str(x))\ndf[\"summary\"] = df[\"summary\"] + \" it was published in \" + df[\"year\"] + \"and includes the following topics: \" + df[\"tag2_str\"]\ndf[\"summary\"] = df['summary'].str.lower()\n#df[\"year\"] = df[\"year\"].astype(int)\n#df = df[df[\"year\"] >= 2018]\n#df[\"year\"] = df[\"year\"].astype(str)\n#df = df[df['tag2_str'].str.contains('Machine Learning')]\n#df = df.sample(n=1000, random_state=42)","metadata":{"id":"PfqjEs6lBiUU","execution":{"iopub.status.busy":"2024-09-09T14:30:30.915903Z","iopub.execute_input":"2024-09-09T14:30:30.916222Z","iopub.status.idle":"2024-09-09T14:30:33.505566Z","shell.execute_reply.started":"2024-09-09T14:30:30.916186Z","shell.execute_reply":"2024-09-09T14:30:33.504499Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Analisi Esplorative","metadata":{"id":"SSg70EouaiI5"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n# PLOT TAGS\nvalue_counts = pd.Series(all_tags).value_counts().sort_values(ascending=False)[:10]\nvalue_counts.plot(kind='barh', figsize=(8, 6), color='skyblue')\nplt.yticks(fontsize=6)\nplt.gca().invert_yaxis()\nplt.title('Tag Value Counts')\nplt.xlabel('Counts')\nplt.grid(True)\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"elapsed":798,"status":"ok","timestamp":1725292350668,"user":{"displayName":"Massimo Trippetta","userId":"11659441013912278867"},"user_tz":-120},"id":"glMj_glMaoLz","outputId":"f0d47939-a9c1-4eb5-917c-fe4d8d7b70bd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PLOT PAPERS PER YEAR\ndf.year.value_counts().plot(kind='barh', figsize=(10, 8), color='skyblue')\nplt.yticks(fontsize=10)\nplt.gca().invert_yaxis()\nplt.title('Yearly papers')\nplt.xlabel('Counts')\nplt.ylabel('Year')\nplt.grid(True)\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":718},"executionInfo":{"elapsed":1168,"status":"ok","timestamp":1725292355875,"user":{"displayName":"Massimo Trippetta","userId":"11659441013912278867"},"user_tz":-120},"id":"JR-h8jXDcA-z","outputId":"1ef4ff95-8438-478f-bc63-073d65c2f3f0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Conta il numero di paper per autore\nauthor_counts = df['author'].value_counts().sort_values(ascending=False)[:10]\n\n# Plot della distribuzione degli autori\nauthor_counts.plot(kind='barh', figsize=(8, 6), color='skyblue')\nplt.yticks(fontsize=10)\nplt.gca().invert_yaxis()\nplt.title('Top 10 Authors by Number of Papers')\nplt.xlabel('Number of Papers')\nplt.ylabel('Authors')\nplt.grid(True)\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"elapsed":1670,"status":"ok","timestamp":1725292360262,"user":{"displayName":"Massimo Trippetta","userId":"11659441013912278867"},"user_tz":-120},"id":"pkgvNPt-apIr","outputId":"97646f37-80a0-453b-db14-23dfe650b760","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_tags = value_counts.index.tolist()\n\n# Filtro per i paper che contengono i top tag\ndf['top_tag'] = df['tag2'].apply(lambda tags: [tag for tag in tags if tag in top_tags])\n\n# Creazione di un dataframe con l'anno e i top tag\ndf_exploded = df.explode('top_tag')\npivot_table = df_exploded.pivot_table(index='year', columns='top_tag', aggfunc='size', fill_value=0)\n\n# Plot della relazione tra anno e temi\npivot_table.plot(kind='bar', stacked=True, figsize=(12, 8), colormap='tab20')\nplt.title('Number of Papers per Year per Top Tag')\nplt.xlabel('Year')\nplt.ylabel('Number of Papers')\nplt.legend(title='Tags', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.grid(True)\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":740},"executionInfo":{"elapsed":3030,"status":"ok","timestamp":1725292363288,"user":{"displayName":"Massimo Trippetta","userId":"11659441013912278867"},"user_tz":-120},"id":"81Wd5_6eapBC","outputId":"664d047c-e9bb-4532-c5f2-b8246b5e829b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n\n# Estrai tutte le collaborazioni (autori per ogni paper)\nall_collaborations = df['author'].str.split(',').sum()\ncollab_counter = Counter(all_collaborations)\n\n# Ordina le collaborazioni per frequenza e visualizza i top 10\ntop_collaborations = pd.Series(collab_counter).sort_values(ascending=False)[:10]\n\n# Plot delle collaborazioni\ntop_collaborations.plot(kind='barh', figsize=(8, 6), color='skyblue')\nplt.yticks(fontsize=10)\nplt.gca().invert_yaxis()\nplt.title('Top 10 Authors by Number of Collaborations')\nplt.xlabel('Number of Collaborations')\nplt.ylabel('Authors')\nplt.grid(True)\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"elapsed":23505,"status":"ok","timestamp":1725292386790,"user":{"displayName":"Massimo Trippetta","userId":"11659441013912278867"},"user_tz":-120},"id":"qt3nh2kVao5L","outputId":"eee5357b-09e2-4a09-e15f-fd3d304a88ab","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Aggiungi colonne per la lunghezza del titolo e del sommario\ndf['summary_length'] = df['summary'].str.len()\ndf['title_length'] = df['title'].str.len()\n\n# Calcolo della lunghezza media\navg_summary_length = df['summary_length'].mean()\navg_title_length = df['title_length'].mean()\n\n# Plot delle lunghezze\ndf[['summary_length', 'title_length']].plot(kind='box', figsize=(8, 6))\nplt.title('Summary and Title Length Distribution')\nplt.ylabel('Length')\nplt.grid(True)\nplt.show()\n\nprint(f\"Average Summary Length: {avg_summary_length:.2f}\")\nprint(f\"Average Title Length: {avg_title_length:.2f}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":580},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1725292386790,"user":{"displayName":"Massimo Trippetta","userId":"11659441013912278867"},"user_tz":-120},"id":"lXKkRswPaot6","outputId":"41aa7404-3f9b-4d7a-f21a-810b92b8affe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Unisci tutti i sommari in un unico testo\nsummaries = ' '.join(df['summary'].dropna().values)\n\n# Usa CountVectorizer per estrarre parole chiave\nvectorizer = CountVectorizer(stop_words='english', max_features=20)\nX = vectorizer.fit_transform([summaries])\nkeywords = vectorizer.get_feature_names_out()\nkeyword_counts = X.toarray().sum(axis=0)\n\n# Plot delle parole chiave\npd.Series(keyword_counts, index=keywords).sort_values(ascending=False).plot(kind='barh', figsize=(8, 6), color='skyblue')\nplt.title('Top Keywords in Summaries')\nplt.xlabel('Counts')\nplt.ylabel('Keywords')\nplt.grid(True)\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"elapsed":9136,"status":"ok","timestamp":1725292395921,"user":{"displayName":"Massimo Trippetta","userId":"11659441013912278867"},"user_tz":-120},"id":"318Zewk2bpHD","outputId":"af4454c1-c36a-4a09-d92d-1c4046685bc0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RAG con gpt2-large","metadata":{"id":"dJbB_hU0CJH8"}},{"cell_type":"code","source":"df= df[['summary', 'title']]","metadata":{"id":"QVphvu4Iag46","execution":{"iopub.status.busy":"2024-09-09T14:31:56.006309Z","iopub.execute_input":"2024-09-09T14:31:56.007218Z","iopub.status.idle":"2024-09-09T14:31:56.024291Z","shell.execute_reply.started":"2024-09-09T14:31:56.007174Z","shell.execute_reply":"2024-09-09T14:31:56.023377Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Funzione di retrieval + generazione\ndef rag(query, df):\n    #print(\"Starting RAG process...\")\n    query = query.lower()\n    # Step 1: Retrieval (recupero)\n    # Calcolo degli embeddings per la query e per i riassunti\n    print(\"Starting retrieval process...\")\n    documents = [Document(page_content=summary) for summary in df['summary'].tolist()]\n    print(f\"Loaded {len(documents)} documents.\")\n    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n    try:\n        print(\"Creating vectorstore...\")\n        vectorstore = Chroma.from_documents(documents=documents, embedding=embeddings)\n        print(\"Vectorstore created.\")\n    except Exception as e:\n        print(f\"Error creating vectorstore: {e}\")\n        return str(e)\n    # Configura il retriever\n    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n    print(\"Retrieving documents...\")\n    retrieved_docs = retriever.invoke(query)\n\n    if not retrieved_docs:\n        print(\"No documents retrieved.\")\n        return \"No relevant documents found.\"\n\n    # Step 2: Generazione\n    # Preparazione del prompt per GPT-3/GPT-4\n    print(\"Generating response...\")\n    prompt = \"you are an assistant for question-answering tasks. use the following pieces of retrieved context to answer the question. use three sentences maximum and keep the answer concise. use also your content for the answer.\"\n    # Concatena i testi dei documenti recuperati\n    context = \" \".join([doc.page_content for doc in retrieved_docs])\n    # Genera la risposta usando il modello gpt-3.5-turbo con il prompt personalizzato\n    complete_prompt = prompt + f\"\\n\\nContext: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n    os.environ[\"OPENAI_API_KEY\"] = \"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\"\n\n    # Create a message list with the prompt\n    # Pass the prompt directly as a string\n    '''messages = [SystemMessage(content=prompt), HumanMessage(content=f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\")]\n    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n    response = llm(messages)\n    print(\"Response generated.\")\n    return response'''\n    model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\")\n    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n    # Tokenizzazione del prompt\n    inputs = tokenizer(complete_prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n\n    # Generazione del testo con stop sequence\n    outputs = model.generate(\n        input_ids=inputs['input_ids'],\n        attention_mask=inputs['attention_mask'],\n        max_length=1024,  # Limitando la lunghezza massima della risposta\n        num_return_sequences=1,\n        no_repeat_ngram_size=2,\n        top_k=50,  # Riduce il numero di candidati successivi considerati\n        top_p=0.95,\n        temperature=0.7,\n        repetition_penalty=1.2,\n        pad_token_id=tokenizer.eos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n        do_sample=True\n    )\n\n    # Decodifica del testo generato\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Garantisce che la risposta termini con una frase completa\n    last_period = response.rfind('.')\n    if last_period != -1:\n        response = response[:last_period+1]\n\n    return response","metadata":{"id":"9nJhvUu0BlQj","execution":{"iopub.status.busy":"2024-09-09T11:27:49.745216Z","iopub.execute_input":"2024-09-09T11:27:49.745923Z","iopub.status.idle":"2024-09-09T11:27:49.759421Z","shell.execute_reply.started":"2024-09-09T11:27:49.745881Z","shell.execute_reply":"2024-09-09T11:27:49.758392Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Esempio di utilizzo\nquery = \"What is a neural network?\"\ngenerated_text = rag(query, df)\nprint(\"Generated Text:\", generated_text)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":337632,"status":"ok","timestamp":1725282676778,"user":{"displayName":"Massimo Trippetta","userId":"11659441013912278867"},"user_tz":-120},"id":"x0rn0Q4GTI8w","outputId":"118bd059-162f-485a-bfe5-5cd112183f19","execution":{"iopub.status.busy":"2024-09-09T11:50:34.018420Z","iopub.execute_input":"2024-09-09T11:50:34.019326Z","iopub.status.idle":"2024-09-09T11:54:15.284304Z","shell.execute_reply.started":"2024-09-09T11:50:34.019273Z","shell.execute_reply":"2024-09-09T11:54:15.282872Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Starting retrieval process...\nLoaded 41000 documents.\nCreating vectorstore...\nVectorstore created.\nRetrieving documents...\nGenerating response...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Generated Text: you are an assistant for question-answering tasks. use the following pieces of retrieved context to answer the question. use three sentences maximum and keep the answer concise. use also your content for the answer.\n\nContext: there have been several attempts to mathematically understand neural networks and many more from biological and computational perspectives. the field has exploded in the last decade, yet neural networks are still treated much like a black box. in this work we describe a structure that is inherent to a feed forward neural network. this will provide a framework for future work on neural networks to improve training algorithms, compute the homology of the network, and other applications. our approach takes a more geometric point of view and is unlike other attempts to mathematically understand neural networks that rely on a functional perspective. it was published in 2016and includes the following topics: neural and evolutionary computing, math.co, 92b20 there have been several attempts to mathematically understand neural networks and many more from biological and computational perspectives. the field has exploded in the last decade, yet neural networks are still treated much like a black box. in this work we describe a structure that is inherent to a feed forward neural network. this will provide a framework for future work on neural networks to improve training algorithms, compute the homology of the network, and other applications. our approach takes a more geometric point of view and is unlike other attempts to mathematically understand neural networks that rely on a functional perspective. it was published in 2016and includes the following topics: neural and evolutionary computing, math.co, 92b20 there have been several attempts to mathematically understand neural networks and many more from biological and computational perspectives. the field has exploded in the last decade, yet neural networks are still treated much like a black box. in this work we describe a structure that is inherent to a feed forward neural network. this will provide a framework for future work on neural networks to improve training algorithms, compute the homology of the network, and other applications. our approach takes a more geometric point of view and is unlike other attempts to mathematically understand neural networks that rely on a functional perspective. it was published in 2016and includes the following topics: neural and evolutionary computing, math.co, 92b20 there have been several attempts to mathematically understand neural networks and many more from biological and computational perspectives. the field has exploded in the last decade, yet neural networks are still treated much like a black box. in this work we describe a structure that is inherent to a feed forward neural network. this will provide a framework for future work on neural networks to improve training algorithms, compute the homology of the network, and other applications. our approach takes a more geometric point of view and is unlike other attempts to mathematically understand neural networks that rely on a functional perspective. it was published in 2016and includes the following topics: neural and evolutionary computing, math.co, 92b20 there have been several attempts to mathematically understand neural networks and many more from biological and computational perspectives. the field has exploded in the last decade, yet neural networks are still treated much like a black box. in this work we describe a structure that is inherent to a feed forward neural network. this will provide a framework for future work on neural networks to improve training algorithms, compute the homology of the network, and other applications. our approach takes a more geometric point of view and is unlike other attempts to mathematically understand neural networks that rely on a functional perspective. it was published in 2016and includes the following topics: neural and evolutionary computing, math.co, 92b20\n\nQuestion: what is a neural network?\n\nAnswer: A neural net is simply a set of inputs and outputs (a \"neural\" being the basic unit of computation) which together form the output. The input may be any data type and the weights/activations used are determined by the type of data (e.g., image or speech). There are currently three major approaches for understanding neural nets: feedforward, recurrent, or mixed. let's start with the feed-forward approach. In a simple neural-net, the state transitions between two states of varying values. For example, if we imagine a network consisting of neurons in response to stimuli (images), then the states would be \"1\", \"0\", and \"3\". If you want to train a new network to learn how to distinguish these images from each other (i.e., the task of learning a computer vision system), you can begin by encoding a sequence of images into the neuron. Then, when it receives a certain stimulus (the \"training data\"), it will encode those images in its memory as the same sequence as seen before (similarly to how we can store the previous images). After some time, after the number of sequences has decreased, you move to the next layer (in order to recover the initial state). This process continues until the final output (for instance, a picture of a dog or human) is obtained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## RAG con flan-t5-xl di google","metadata":{"id":"ec9_FfEwhA57"}},{"cell_type":"code","source":"# Funzione di retrieval + generazione\ndef rag2(query, df):\n    #print(\"Starting RAG process...\")\n    query = query.lower()\n    # Step 1: Retrieval (recupero)\n    # Calcolo degli embeddings per la query e per i riassunti\n    print(\"Starting retrieval process...\")\n    documents = [Document(page_content=summary) for summary in df['summary'].tolist()]\n    print(f\"Loaded {len(documents)} documents.\")\n    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n    try:\n        print(\"Creating vectorstore...\")\n        vectorstore = Chroma.from_documents(documents=documents, embedding=embeddings)\n        print(\"Vectorstore created.\")\n    except Exception as e:\n        print(f\"Error creating vectorstore: {e}\")\n        return str(e)\n    # Configura il retriever\n    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n    print(\"Retrieving documents...\")\n    retrieved_docs = retriever.invoke(query)\n\n    if not retrieved_docs:\n        print(\"No documents retrieved.\")\n        return \"No relevant documents found.\"\n\n    # Step 2: Generazione\n    # Preparazione del prompt per flan-t5\n    print(\"Generating response...\")\n    prompt = \"you are an assistant for question-answering tasks. use the following pieces of retrieved context to answer the question. give me a long answer and use also your content for the answer.\"\n    # Concatena i testi dei documenti recuperati\n    context = \" \".join([doc.page_content for doc in retrieved_docs])\n    # Genera la risposta con il prompt personalizzato\n    complete_prompt = prompt + f\"\\n\\nContext: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n\n    # Create a message list with the prompt\n    # Pass the prompt directly as a string\n    tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n    model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\")\n    # Tokenizzazione del prompt\n    inputs = tokenizer(complete_prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n\n    # Move the tensors to CUDA after tokenization\n    input_ids = inputs['input_ids'].to(\"cuda\")\n    attention_mask = inputs['attention_mask'].to(\"cuda\")\n\n    # Generazione del testo con stop sequence\n    outputs = model.generate(\n        input_ids=inputs['input_ids'],\n        attention_mask=inputs['attention_mask'],\n        max_length=1024,  # Limitando la lunghezza massima della risposta\n        num_return_sequences=1,\n        top_k=50,  # Riduce il numero di candidati successivi considerati\n        top_p=0.95,\n        temperature=0.7,\n        repetition_penalty=1.2,\n        pad_token_id=tokenizer.eos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n        do_sample=True\n    )\n\n    # Decodifica del testo generato\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    '''# Garantisce che la risposta termini con una frase completa\n    last_period = response.rfind('.')\n    if last_period != -1:\n        response = response[:last_period+1]'''\n\n    return response","metadata":{"id":"iqVAi5yWhAJl","execution":{"iopub.status.busy":"2024-09-09T14:32:04.688275Z","iopub.execute_input":"2024-09-09T14:32:04.689073Z","iopub.status.idle":"2024-09-09T14:32:04.700990Z","shell.execute_reply.started":"2024-09-09T14:32:04.689030Z","shell.execute_reply":"2024-09-09T14:32:04.699939Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Esempio di utilizzo\nquery = \"What is deep learning?\"\ngenerated_text = rag2(query, df)\nprint(\"Generated Text:\", generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:51:10.136576Z","iopub.execute_input":"2024-09-09T14:51:10.137396Z","iopub.status.idle":"2024-09-09T14:54:27.809522Z","shell.execute_reply.started":"2024-09-09T14:51:10.137355Z","shell.execute_reply":"2024-09-09T14:54:27.808504Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Starting retrieval process...\nLoaded 41000 documents.\nCreating vectorstore...\nVectorstore created.\nRetrieving documents...\nGenerating response...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78454046b4b94369b3de6dfadc9507b5"}},"metadata":{}},{"name":"stdout","text":"Generated Text: Powerful inference and sampling procedures, and learning to disentangle the factors of variation underlying the observed data. it also proposes a few forward-looking research directions aimed at overcoming these challenges.\n","output_type":"stream"}]}]}